{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5VxcNxXH0Z0tovQk2+ujW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17c86fb3a8044485996087bc02b8b305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cddba24441d477488440ef3c0609ac4",
              "IPY_MODEL_ed9d562de6da4f37bc4e2cb9baa7962b",
              "IPY_MODEL_48e6f5f0192d44148b0c79f516ad1001"
            ],
            "layout": "IPY_MODEL_0cce87d77acb45bdba5a768c7f75717b"
          }
        },
        "1cddba24441d477488440ef3c0609ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40f3b722b9324f5986a5ad246c5052fc",
            "placeholder": "​",
            "style": "IPY_MODEL_298639df74b94811bd761cec93fdf4b4",
            "value": "Extracting Features: 100%"
          }
        },
        "ed9d562de6da4f37bc4e2cb9baa7962b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2018744d94945bf856cfe81e2b3444c",
            "max": 39,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_014b0a058e404d31aa2cfa81b58f4f8f",
            "value": 39
          }
        },
        "48e6f5f0192d44148b0c79f516ad1001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bfa4175deea47e8b8f8c7250d4872d7",
            "placeholder": "​",
            "style": "IPY_MODEL_a9f35dac2fa746f4953928cbb0cf1013",
            "value": " 39/39 [03:52&lt;00:00,  4.33s/it]"
          }
        },
        "0cce87d77acb45bdba5a768c7f75717b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40f3b722b9324f5986a5ad246c5052fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "298639df74b94811bd761cec93fdf4b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2018744d94945bf856cfe81e2b3444c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "014b0a058e404d31aa2cfa81b58f4f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9bfa4175deea47e8b8f8c7250d4872d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9f35dac2fa746f4953928cbb0cf1013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appababba/USDA/blob/main/ActiveLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "17c86fb3a8044485996087bc02b8b305",
            "1cddba24441d477488440ef3c0609ac4",
            "ed9d562de6da4f37bc4e2cb9baa7962b",
            "48e6f5f0192d44148b0c79f516ad1001",
            "0cce87d77acb45bdba5a768c7f75717b",
            "40f3b722b9324f5986a5ad246c5052fc",
            "298639df74b94811bd761cec93fdf4b4",
            "e2018744d94945bf856cfe81e2b3444c",
            "014b0a058e404d31aa2cfa81b58f4f8f",
            "9bfa4175deea47e8b8f8c7250d4872d7",
            "a9f35dac2fa746f4953928cbb0cf1013"
          ]
        },
        "id": "zUSCp5F38-wA",
        "outputId": "096e6b60-971f-49c5-e89b-1d46c45b8b99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/hdbscan/robust_single_linkage_.py:175: SyntaxWarning: invalid escape sequence '\\{'\n",
            "  $max \\{ core_k(a), core_k(b), 1/\\alpha d(a,b) \\}$.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Copying data from /content/drive/Shared drives/USDA-Summer2025/data/Exported_Images to /content/local_data/Exported_Images...\n",
            "Data copy complete.\n",
            "Copying data from /content/drive/Shared drives/USDA-Summer2025/data/Exported_Masks to /content/local_data/Exported_Masks...\n",
            "Data copy complete.\n",
            "Initializing model (plain ResNet50 UNet)...\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 342MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling model for performance...\n",
            "Model compiled successfully.\n",
            "\n",
            "Found 1217 total images for feature extraction.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting Features:   0%|          | 0/39 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17c86fb3a8044485996087bc02b8b305"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extraction complete.\n",
            "Feature vector shape: (1217, 2048), Total paths: 1217\n",
            "\n",
            "Performing HDBSCAN clustering on feature vectors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clustering complete. Found 0 clusters and 1217 noise points (outliers).\n",
            "\n",
            "Selected the top 20 most informative samples to label next:\n",
            "  1. IMG_5471_092846_20250813_section14.jpg\n",
            "  2. IMG_5150_093149_20250813_section15.jpg\n",
            "  3. IMG_4606_085228_20250813_section1.jpg\n",
            "  4. IMG_5163_093151_20250813_section15.jpg\n",
            "  5. IMG_4601_085226_20250813_section1.jpg\n",
            "\n",
            "--- Initializing Active Learning Loop ---\n",
            "Resuming from checkpoint: al_ckpt_13.pt\n",
            "Starting active learning from iteration: 15\n",
            "\n",
            "--- Iteration 15/15 | Labeled Samples: 300 ---\n",
            "Validation IoU: 0.4551\n",
            "Selecting next batch of samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Active learning complete. Checkpoints saved in: /content/drive/MyDrive/active_learning_ckpts\n"
          ]
        }
      ],
      "source": [
        "# --- install stuff (colab) ---\n",
        "!pip install -q segmentation-models-pytorch==0.3.3 albumentations==1.4.7 hdbscan pretrainedmodels efficientnet_pytorch --no-deps\n",
        "\n",
        "# --- imports ---\n",
        "import os, random, shutil, math, glob, gc, pickle, json, copy, sys\n",
        "import numpy as np, pandas as pd, cv2, torch\n",
        "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from contextlib import nullcontext\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "import hdbscan\n",
        "\n",
        "# --- env + reproducibility ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "if hasattr(torch.backends, \"cuda\") and hasattr(torch.backends.cuda, \"matmul\"):\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "if hasattr(torch.backends, \"cudnn\"):\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "use_amp = torch.cuda.is_available() and hasattr(torch.cuda, 'amp') and torch.cuda.is_bf16_supported()\n",
        "amp_dtype = torch.bfloat16 if use_amp else torch.float32\n",
        "\n",
        "# --- mount drive ---\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except Exception as e:\n",
        "    print(f\"drive mount fail: {e}\")\n",
        "\n",
        "# --- paths ---\n",
        "BASE_DRIVE = \"/content/drive/Shared drives/USDA-Summer2025\"\n",
        "DATA_DIR   = os.path.join(BASE_DRIVE, \"data\")\n",
        "IMG_DRIVE  = os.path.join(DATA_DIR, \"Exported_Images\")\n",
        "MSK_DRIVE  = os.path.join(DATA_DIR, \"Exported_Masks\")\n",
        "MODELS_DIR = os.path.join(BASE_DRIVE, \"models\")\n",
        "LOCAL_ROOT = \"/content/local_data\"\n",
        "IMG_LOCAL  = os.path.join(LOCAL_ROOT, \"Exported_Images\")\n",
        "MSK_LOCAL  = os.path.join(LOCAL_ROOT, \"Exported_Masks\")\n",
        "os.makedirs(LOCAL_ROOT, exist_ok=True)\n",
        "\n",
        "# --- pull data local if needed ---\n",
        "def ensure_local_data(src, dst):\n",
        "    if os.path.isdir(dst) and any(os.scandir(dst)):\n",
        "        print(f\"local data ok: {dst}\")\n",
        "        return\n",
        "    print(f\"copying from {src} to {dst}...\")\n",
        "    shutil.copytree(src, dst)\n",
        "    print(\"done.\")\n",
        "\n",
        "ensure_local_data(IMG_DRIVE, IMG_LOCAL)\n",
        "ensure_local_data(MSK_DRIVE, MSK_LOCAL)\n",
        "\n",
        "# --- datasets ---\n",
        "class SegDataset(Dataset):\n",
        "    def __init__(self, image_paths, mask_dir, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def _get_mask_path(self, img_path):\n",
        "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        main = os.path.join(self.mask_dir, f\"{base}_mask.png\")\n",
        "        if os.path.exists(main):\n",
        "            return main\n",
        "        for ext in ('.png', '.jpg', '.jpeg', '.tif', '.tiff'):\n",
        "            alt = os.path.join(self.mask_dir, base + ext)\n",
        "            if os.path.exists(alt):\n",
        "                return alt\n",
        "        raise FileNotFoundError(f\"no mask for {img_path}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        mask_path = self._get_mask_path(img_path)\n",
        "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "        mask = (cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) > 0).astype(np.uint8)\n",
        "        if self.transform:\n",
        "            t = self.transform(image=img, mask=mask)\n",
        "            img, mask = t['image'], t['mask']\n",
        "        if isinstance(mask, torch.Tensor) and mask.ndim == 2:\n",
        "            mask = mask.unsqueeze(0)\n",
        "        return img, mask.float(), img_path\n",
        "\n",
        "class InferenceDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)['image']\n",
        "        return img, img_path\n",
        "\n",
        "# --- plain UNet ---\n",
        "class UNetPlain(nn.Module):\n",
        "    def __init__(self, encoder='resnet50', in_ch=3, classes=1):\n",
        "        super().__init__()\n",
        "        self.net = smp.Unet(encoder_name=encoder, encoder_weights='imagenet',\n",
        "                            in_channels=in_ch, classes=classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def extract_features(self, x):\n",
        "        return self.net.encoder(x)[-1]\n",
        "\n",
        "# --- augments ---\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# --- init model ---\n",
        "print('init UNet (resnet50)')\n",
        "model = UNetPlain('resnet50').to(DEVICE, memory_format=torch.channels_last)\n",
        "model.eval()\n",
        "try:\n",
        "    print('compiling...')\n",
        "    model = torch.compile(model, mode='max-autotune')\n",
        "    print('compiled ok')\n",
        "except Exception as e:\n",
        "    print(f'compile skipped: {e}')\n",
        "\n",
        "# --- feature extraction ---\n",
        "all_image_paths = [os.path.join(IMG_LOCAL, f) for f in os.listdir(IMG_LOCAL) if not f.startswith('.')]\n",
        "print(f'found {len(all_image_paths)} imgs')\n",
        "\n",
        "dataset = InferenceDataset(all_image_paths, val_transform)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=(DEVICE.type=='cuda'))\n",
        "\n",
        "all_feats, all_paths = [], []\n",
        "with torch.no_grad():\n",
        "    for imgs, paths in tqdm(loader, desc='extracting'):\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        with torch.autocast(device_type='cuda', dtype=amp_dtype, enabled=use_amp):\n",
        "            f = model.extract_features(imgs)\n",
        "            pooled = F.adaptive_avg_pool2d(f, (1,1)).flatten(1)\n",
        "            normed = F.normalize(pooled, p=2, dim=1)\n",
        "        all_feats.append(normed.cpu().numpy())\n",
        "        all_paths.extend(paths)\n",
        "\n",
        "feats = np.vstack(all_feats) if all_feats else np.empty((0,2048))\n",
        "print('features ready', feats.shape)\n",
        "\n",
        "# --- clustering ---\n",
        "if feats.shape[0] > 0:\n",
        "    print('running HDBSCAN...')\n",
        "    cl = hdbscan.HDBSCAN(min_cluster_size=15, prediction_data=True, core_dist_n_jobs=-1)\n",
        "    labels = cl.fit_predict(feats)\n",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    n_noise = int(np.sum(labels == -1))\n",
        "    print(f'{n_clusters} clusters, {n_noise} outliers')\n",
        "    outlier_scores = cl.outlier_scores_\n",
        "    scored = sorted(zip(outlier_scores, all_paths), key=lambda x: x[0], reverse=True)\n",
        "else:\n",
        "    print('no features, skip clustering')\n",
        "    scored = []\n",
        "\n",
        "def select_next_batch(scored_samples, labeled, batch_size=20):\n",
        "    picks, seen = [], set(labeled)\n",
        "    for _, p in scored_samples:\n",
        "        if p not in seen:\n",
        "            picks.append(p)\n",
        "            if len(picks) >= batch_size:\n",
        "                break\n",
        "    return picks\n",
        "\n",
        "labeled_paths = []\n",
        "BATCH_SIZE = 20\n",
        "first_batch = select_next_batch(scored, labeled_paths, BATCH_SIZE) if scored else []\n",
        "print(f'next {len(first_batch)} to label')\n",
        "for i,p in enumerate(first_batch[:5]):\n",
        "    print(f'  {i+1}. {os.path.basename(p)}')\n",
        "\n",
        "# --- active learning loop setup ---\n",
        "print('\\nstarting AL loop')\n",
        "CKPT_DIR = '/content/drive/MyDrive/active_learning_ckpts'\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "NUM_WORKERS = 0\n",
        "PIN_MEMORY = (DEVICE.type == 'cuda')\n",
        "\n",
        "ckpt_files = sorted(glob.glob(f'{CKPT_DIR}/al_ckpt_*.pt'), key=os.path.getmtime, reverse=True)\n",
        "latest_ckpt = ckpt_files[0] if ckpt_files else None\n",
        "\n",
        "current_model = UNetPlain('resnet50').to(DEVICE, memory_format=torch.channels_last)\n",
        "optimizer = optim.Adam(current_model.parameters(), lr=1e-4)\n",
        "\n",
        "start_iter = 0\n",
        "al_labeled, al_unlabeled, history = [], [], []\n",
        "\n",
        "if latest_ckpt:\n",
        "    try:\n",
        "        print(f'resuming from {os.path.basename(latest_ckpt)}')\n",
        "        ckpt = torch.load(latest_ckpt, map_location=DEVICE)\n",
        "        cur = current_model.state_dict()\n",
        "        compat = {k:v for k,v in ckpt['model_state'].items() if k in cur and v.shape==cur[k].shape}\n",
        "        current_model.load_state_dict(compat, strict=False)\n",
        "        try:\n",
        "            optimizer.load_state_dict(ckpt['optimizer_state'])\n",
        "        except Exception:\n",
        "            pass\n",
        "        al_labeled = ckpt.get('al_labeled_pool', [])\n",
        "        al_unlabeled = ckpt.get('al_unlabeled_pool', [])\n",
        "        history = ckpt.get('active_learning_history', [])\n",
        "        start_iter = ckpt.get('iter_idx', -1) + 1\n",
        "    except Exception as e:\n",
        "        print(f'bad checkpoint, starting fresh: {e}')\n",
        "\n",
        "if not al_labeled:\n",
        "    print('making new pools...')\n",
        "    FULL = [os.path.join(IMG_LOCAL, f) for f in os.listdir(IMG_LOCAL) if not f.startswith('.')]\n",
        "    ratios = []\n",
        "    helper = SegDataset([], MSK_LOCAL)\n",
        "    for p in tqdm(FULL, desc='mask ratios'):\n",
        "        try:\n",
        "            m = cv2.imread(helper._get_mask_path(p), 0)\n",
        "            ratios.append(float(np.mean(m>0)) if m is not None else 0)\n",
        "        except FileNotFoundError:\n",
        "            ratios.append(0)\n",
        "    df = pd.DataFrame({'path':FULL, 'ratio':ratios})\n",
        "    df['ratio_bin'] = pd.qcut(df['ratio'], q=5, labels=False, duplicates='drop')\n",
        "    train_val, test_paths, _, _ = train_test_split(df['path'].tolist(), df['ratio_bin'].tolist(), test_size=0.2, random_state=SEED, stratify=df['ratio_bin'].tolist())\n",
        "    random.shuffle(train_val)\n",
        "    al_labeled, al_unlabeled, history = train_val[:20], train_val[20:], []\n",
        "else:\n",
        "    if 'test_paths' not in globals():\n",
        "        _, test_paths = train_test_split([os.path.join(IMG_LOCAL, f) for f in os.listdir(IMG_LOCAL)], test_size=0.2, random_state=SEED)\n",
        "\n",
        "# --- dataloaders + train/val ---\n",
        "def make_loader(paths, tfm, bs, shuffle=False):\n",
        "    ds = SegDataset(paths, MSK_LOCAL, transform=tfm)\n",
        "    return DataLoader(ds, batch_size=bs, shuffle=shuffle, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "def train_one_iter(model, loader, epochs=5):\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for imgs, masks, _ in loader:\n",
        "            imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.autocast(device_type='cuda', dtype=amp_dtype, enabled=use_amp):\n",
        "                preds = model(imgs)\n",
        "                loss = nn.BCEWithLogitsLoss()(preds, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, max_batches=10):\n",
        "    if len(loader.dataset)==0: return 0.0\n",
        "    model.eval()\n",
        "    total, n = 0.0, 0\n",
        "    for i,(imgs, masks, _) in enumerate(loader):\n",
        "        if i>=max_batches: break\n",
        "        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
        "        with torch.autocast(device_type='cuda', dtype=amp_dtype, enabled=use_amp):\n",
        "            preds = torch.sigmoid(model(imgs))\n",
        "        inter = (preds*masks).sum()\n",
        "        union = (preds+masks).sum() - inter\n",
        "        total += (inter/(union+1e-6)).item()\n",
        "        n+=1\n",
        "    return total/max(1,n)\n",
        "\n",
        "# --- embeddings + batch select ---\n",
        "EMB_CACHE = {}\n",
        "PCA_MODEL = None\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_embeddings(paths, bs=64):\n",
        "    to_do = [p for p in paths if p not in EMB_CACHE]\n",
        "    if to_do:\n",
        "        ds = InferenceDataset(to_do, val_transform)\n",
        "        loader = DataLoader(ds, batch_size=bs, shuffle=False, num_workers=0)\n",
        "        current_model.eval()\n",
        "        for imgs, pths in loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            with torch.autocast(device_type='cuda', dtype=amp_dtype, enabled=use_amp):\n",
        "                f = F.normalize(F.adaptive_avg_pool2d(current_model.extract_features(imgs), (1,1)).flatten(1))\n",
        "            for p,fv in zip(pths, f.cpu().numpy()):\n",
        "                EMB_CACHE[p] = fv\n",
        "    return np.stack([EMB_CACHE[p] for p in paths]) if paths else np.empty((0,1))\n",
        "\n",
        "def pick_next(unlabeled, batch_size=20, candidate_k=500):\n",
        "    global PCA_MODEL\n",
        "    if not unlabeled: return []\n",
        "    cand = random.sample(unlabeled, k=min(candidate_k,len(unlabeled)))\n",
        "    feats = get_embeddings(cand)\n",
        "    if PCA_MODEL is None and feats.shape[0]>=64:\n",
        "        PCA_MODEL = PCA(n_components=64, random_state=SEED).fit(feats)\n",
        "    reduced = PCA_MODEL.transform(feats) if PCA_MODEL else feats\n",
        "    scores = hdbscan.HDBSCAN(min_cluster_size=15).fit(reduced).outlier_scores_\n",
        "    ranked = [c for _,c in sorted(zip(scores,cand), key=lambda x:x[0], reverse=True)]\n",
        "    return ranked[:batch_size]\n",
        "\n",
        "# --- main loop ---\n",
        "NUM_ITERS = 15\n",
        "EPOCHS_PER = 5\n",
        "print(f'starting from iter {start_iter+1}')\n",
        "\n",
        "for i in range(start_iter, NUM_ITERS):\n",
        "    n_labeled = len(al_labeled)\n",
        "    print(f'iter {i+1}/{NUM_ITERS} | {n_labeled} labeled')\n",
        "    train_loader = make_loader(al_labeled, train_transform, 32, shuffle=True)\n",
        "    train_one_iter(current_model, train_loader, epochs=EPOCHS_PER)\n",
        "\n",
        "    test_loader = make_loader(test_paths, val_transform, 64)\n",
        "    val_iou = validate(current_model, test_loader)\n",
        "    history.append({'num_labeled': n_labeled, 'iou': val_iou})\n",
        "    print(f'val IoU: {val_iou:.4f}')\n",
        "\n",
        "    if not al_unlabeled:\n",
        "        print('no unlabeled left')\n",
        "        break\n",
        "\n",
        "    print('selecting next batch...')\n",
        "    new_batch = pick_next(al_unlabeled, BATCH_SIZE)\n",
        "\n",
        "    al_labeled.extend(new_batch)\n",
        "    al_unlabeled = [p for p in al_unlabeled if p not in new_batch]\n",
        "\n",
        "    torch.save({\n",
        "        'iter_idx': i,\n",
        "        'model_state': current_model.state_dict(),\n",
        "        'optimizer_state': optimizer.state_dict(),\n",
        "        'al_labeled_pool': al_labeled,\n",
        "        'al_unlabeled_pool': al_unlabeled,\n",
        "        'active_learning_history': history\n",
        "    }, f'{CKPT_DIR}/al_ckpt_{i}.pt')\n",
        "\n",
        "    del train_loader, test_loader\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(f'AL done. checkpoints in {CKPT_DIR}')\n"
      ]
    }
  ]
}
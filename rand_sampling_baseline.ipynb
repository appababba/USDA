{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdkI4sQ28dMtOfPDA9XjfI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appababba/USDA/blob/main/rand_sampling_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2AUTlF-BSYM"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# SETUP: DEPENDENCIES AND ENVIRONMENT\n",
        "# =============================================\n",
        "# --- Package Installation ---\n",
        "!pip install -q segmentation-models-pytorch==0.3.3 albumentations==1.4.7 hdbscan --no-deps\n",
        "\n",
        "# --- Core Imports ---\n",
        "import os, random, shutil, math, glob, gc, pickle, json, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from contextlib import nullcontext\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "import hdbscan\n",
        "\n",
        "# --- Environment Configuration ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PIN_MEMORY = (DEVICE.type == \"cuda\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# --- Reproducibility ---\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# --- AMP Configuration ---\n",
        "use_amp = torch.cuda.is_available() and hasattr(torch.cuda, 'amp') and torch.cuda.is_bf16_supported()\n",
        "amp_dtype = torch.bfloat16 if use_amp else torch.float32\n",
        "\n",
        "# --- Suppress specific warnings ---\n",
        "warnings.filterwarnings(\"ignore\", \".*force_all_finite.*\")\n",
        "\n",
        "print(\"Setup complete. Environment configured.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# DATA MANAGEMENT: PATHS AND LOCAL MIRRORING\n",
        "# =============================================\n",
        "# --- Google Drive Mount ---\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "\n",
        "# --- Path Definitions ---\n",
        "BASE_DRIVE = \"/content/drive/Shared drives/USDA-Summer2025\"\n",
        "DATA_DIR = os.path.join(BASE_DRIVE, \"data\")\n",
        "IMG_DRIVE = os.path.join(DATA_DIR, \"Exported_Images\")\n",
        "MSK_DRIVE = os.path.join(DATA_DIR, \"Exported_Masks\")\n",
        "MODELS_DIR = os.path.join(BASE_DRIVE, \"models\")\n",
        "LOCAL_ROOT = \"/content/local_data\"\n",
        "IMG_LOCAL = os.path.join(LOCAL_ROOT, \"Exported_Images\")\n",
        "MSK_LOCAL = os.path.join(LOCAL_ROOT, \"Exported_Masks\")\n",
        "CKPT_DIR = '/content/drive/MyDrive/active_learning_ckpts'\n",
        "os.makedirs(LOCAL_ROOT, exist_ok=True)\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "# --- Data Mirroring Function ---\n",
        "def ensure_local_data(source_dir, dest_dir):\n",
        "    \"\"\"Copies data from G-Drive to local Colab storage if not already present.\"\"\"\n",
        "    if os.path.isdir(dest_dir) and any(os.scandir(dest_dir)):\n",
        "        print(f\"Using existing local data at: {dest_dir}\")\n",
        "    else:\n",
        "        print(f\"Copying data from {source_dir} to {dest_dir}...\")\n",
        "        shutil.copytree(source_dir, dest_dir)\n",
        "        print(\"Data copy complete.\")\n",
        "\n",
        "ensure_local_data(IMG_DRIVE, IMG_LOCAL)\n",
        "ensure_local_data(MSK_DRIVE, MSK_LOCAL)"
      ],
      "metadata": {
        "id": "eaESBZ4YBW0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# CORE COMPONENTS: DATASET AND MODEL CLASSES\n",
        "# =============================================\n",
        "# --- Dataset Classes ---\n",
        "class SegDataset(Dataset):\n",
        "    def __init__(self, image_paths, mask_dir, transform=None):\n",
        "        self.image_paths, self.mask_dir, self.transform = image_paths, mask_dir, transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def _get_mask_path(self, img_path):\n",
        "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        p1 = os.path.join(self.mask_dir, f\"{base}_mask.png\")\n",
        "        if os.path.exists(p1): return p1\n",
        "        for ext in ('.png', '.jpg', '.jpeg', '.tif', '.tiff'):\n",
        "            p2 = os.path.join(self.mask_dir, base + ext)\n",
        "            if os.path.exists(p2): return p2\n",
        "        raise FileNotFoundError(f\"Mask not found for image: {img_path}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ip = self.image_paths[idx]\n",
        "        mp = self._get_mask_path(ip)\n",
        "        img = cv2.cvtColor(cv2.imread(ip), cv2.COLOR_BGR2RGB)\n",
        "        msk = (cv2.imread(mp, cv2.IMREAD_GRAYSCALE) > 0).astype(np.uint8)\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=img, mask=msk)\n",
        "            img, msk = transformed[\"image\"], transformed[\"mask\"]\n",
        "        if isinstance(msk, torch.Tensor) and msk.ndim == 2:\n",
        "            msk = msk.unsqueeze(0)\n",
        "        return img, msk.float(), ip\n",
        "\n",
        "class InferenceDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths, self.transform = image_paths, transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        return image, img_path\n",
        "\n",
        "# --- Model Architecture ---\n",
        "class GaborStem(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.k, self.gamma, self.magnitude = cfg[\"kernel_size\"], cfg[\"gamma\"], cfg[\"magnitude\"]\n",
        "        ax = torch.arange(-(self.k // 2), self.k // 2 + 1, dtype=torch.float32)\n",
        "        X, Y = torch.meshgrid(ax, ax, indexing='xy')\n",
        "\n",
        "        phases = torch.tensor(cfg[\"phases\"], dtype=torch.float32)\n",
        "        lambdas = torch.tensor(cfg[\"wavelengths\"], dtype=torch.float32)\n",
        "        sigmas = torch.tensor(cfg[\"sigmas\"], dtype=torch.float32)\n",
        "        thetas = torch.linspace(0, math.pi, steps=cfg[\"orientations\"], dtype=torch.float32)\n",
        "\n",
        "        base = torch.tensor([(t.item(), s.item(), l.item(), p.item())\n",
        "                             for l, s in zip(lambdas, sigmas) for t in thetas for p in phases])\n",
        "\n",
        "        theta, sigma, lambd, phase = [p.view(-1, 1, 1) for p in base.T]\n",
        "\n",
        "        Xp = X * torch.cos(theta) + Y * torch.sin(theta)\n",
        "        Yp = -X * torch.sin(theta) + Y * torch.cos(theta)\n",
        "\n",
        "        gauss = torch.exp(-(Xp**2 + (torch.as_tensor(self.gamma) * Yp)**2) / (2 * sigma**2))\n",
        "        carrier = torch.cos(2 * math.pi * Xp / lambd + phase)\n",
        "        g = gauss * carrier\n",
        "        g = g - g.mean(dim=(1, 2), keepdim=True)\n",
        "        g = g / (g.square().sum(dim=(1, 2), keepdim=True).sqrt() + 1e-8)\n",
        "\n",
        "        self.register_buffer('kernels', g)\n",
        "        self.register_buffer('phases_buf', phases)\n",
        "\n",
        "        num_per_phase = lambdas.numel() * cfg[\"orientations\"]\n",
        "        self.out_channels = num_per_phase if (self.magnitude and len(phases) == 2) else base.shape[0]\n",
        "        self.norm = nn.InstanceNorm2d(self.out_channels, affine=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = 0.299 * x[:, 0:1] + 0.587 * x[:, 1:2] + 0.114 * x[:, 2:3]\n",
        "        K = self.kernels.to(x.device, x.dtype)\n",
        "        if self.magnitude and self.phases_buf.numel() == 2:\n",
        "            k_cos, k_sin = K[0::2].unsqueeze(1), K[1::2].unsqueeze(1)\n",
        "            rc = F.conv2d(y, k_cos, padding=self.k // 2)\n",
        "            rs = F.conv2d(y, k_sin, padding=self.k // 2)\n",
        "            feats = torch.sqrt(rc**2 + rs**2 + 1e-8)\n",
        "        else:\n",
        "            feats = F.conv2d(y, K.unsqueeze(1), padding=self.k // 2)\n",
        "        return self.norm(feats)\n",
        "\n",
        "class UNetWithGabor_Adapted(nn.Module):\n",
        "    def __init__(self, gcfg, in_img_ch=3):\n",
        "        super().__init__()\n",
        "        self.use_gabor, self.mode = gcfg[\"enabled\"], gcfg[\"mode\"]\n",
        "        self.gabor = GaborStem(gcfg) if self.use_gabor else None\n",
        "        g_out = self.gabor.out_channels if self.gabor else 0\n",
        "        in_ch = (in_img_ch + g_out) if (self.use_gabor and self.mode == \"concat\") else (g_out if self.use_gabor else in_img_ch)\n",
        "\n",
        "        if in_ch == 3:\n",
        "            self.adapter = nn.Identity()\n",
        "        else:\n",
        "            mid_ch = max(16, in_ch // 4)\n",
        "            self.adapter = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, mid_ch, 1, bias=False),\n",
        "                nn.BatchNorm2d(mid_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(mid_ch, 3, 1, bias=False)\n",
        "            )\n",
        "        self.net = smp.Unet(encoder_name='resnet50', encoder_weights='imagenet', in_channels=3, classes=1)\n",
        "\n",
        "    def _prep(self, x):\n",
        "        if self.gabor:\n",
        "            g = self.gabor(x)\n",
        "            x = torch.cat([x, g], dim=1) if self.mode == \"concat\" else g\n",
        "        return self.adapter(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(self._prep(x))\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        x = self._prep(x)\n",
        "        return self.net.encoder(x)[-1]\n",
        "\n",
        "print(\"Core components defined.\")"
      ],
      "metadata": {
        "id": "nXkgPa8-BYgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# EXPERIMENT 1: RANDOM SAMPLING BASELINE\n",
        "# =============================================\n",
        "print(\"\\n--- Starting Random Sampling Baseline Experiment ---\")\n",
        "\n",
        "# --- Configuration ---\n",
        "GABOR_CFG = {\"enabled\":True, \"mode\":\"concat\", \"kernel_size\":15, \"orientations\":8, \"wavelengths\":[4.0,8.0,12.0], \"sigmas\":[1.5,3.0,4.5], \"phases\":[0,np.pi/2], \"gamma\":0.5, \"magnitude\":True}\n",
        "train_transform = A.Compose([A.Resize(256, 256), A.HorizontalFlip(p=0.5), A.RandomBrightnessContrast(p=0.2), A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]), ToTensorV2()])\n",
        "val_transform = A.Compose([A.Resize(256, 256), A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]), ToTensorV2()])\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "CPU_COUNT = os.cpu_count() or 2\n",
        "NUM_WORKERS = max(2, min(8, CPU_COUNT // 2))\n",
        "\n",
        "# --- Dataloader Functions ---\n",
        "def make_loader(paths, transform, batch_size, shuffle=False):\n",
        "    ds = SegDataset(paths, MSK_LOCAL, transform=transform)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, persistent_workers=(NUM_WORKERS > 0))\n",
        "\n",
        "# --- Checkpoint and Pool Initialization ---\n",
        "ckpt_files = sorted(glob.glob(f\"{CKPT_DIR}/rand_ckpt_*.pt\"), key=os.path.getmtime, reverse=True)\n",
        "model_rand = UNetWithGabor_Adapted(gcfg=GABOR_CFG)\n",
        "optimizer_rand = optim.AdamW(model_rand.parameters(), lr=1e-4, fused=torch.cuda.is_available())\n",
        "\n",
        "if ckpt_files:\n",
        "    print(f\"Resuming from checkpoint: {os.path.basename(ckpt_files[0])}\")\n",
        "    ckpt = torch.load(ckpt_files[0], map_location=DEVICE)\n",
        "    model_rand.load_state_dict(ckpt['model_state'])\n",
        "    optimizer_rand.load_state_dict(ckpt['optimizer_state'])\n",
        "    rand_labeled_pool, rand_unlabeled_pool, random_history, start_iter_rand = ckpt['rand_labeled_pool'], ckpt['rand_unlabeled_pool'], ckpt['random_history'], ckpt['iter_idx'] + 1\n",
        "else:\n",
        "    print(\"No checkpoint found. Creating stratified data pools...\")\n",
        "    start_iter_rand = 0\n",
        "    all_paths = [os.path.join(IMG_LOCAL, f) for f in os.listdir(IMG_LOCAL) if not f.startswith('.')]\n",
        "    ratios = [np.mean(cv2.imread(SegDataset([], MSK_LOCAL)._get_mask_path(p), 0) > 0) for p in tqdm(all_paths, \"Calculating Ratios\")]\n",
        "    df = pd.DataFrame({'path': all_paths, 'ratio': ratios})\n",
        "    df['ratio_bin'] = pd.qcut(df['ratio'], q=5, labels=False, duplicates='drop')\n",
        "    train_val_paths, test_paths, _, _ = train_test_split(df['path'].tolist(), df['ratio_bin'].tolist(), test_size=0.2, random_state=SEED, stratify=df['ratio_bin'].tolist())\n",
        "    random.shuffle(train_val_paths)\n",
        "    rand_labeled_pool, rand_unlabeled_pool, random_history = train_val_paths[:20], train_val_paths[20:], []\n",
        "\n",
        "model_rand.to(DEVICE, memory_format=torch.channels_last)\n",
        "try:\n",
        "    print(\"Compiling model...\")\n",
        "    model_rand = torch.compile(model_rand, mode=\"max-autotune\")\n",
        "except Exception as e:\n",
        "    print(f\"torch.compile skipped: {e}\")\n",
        "\n",
        "# --- Training and Validation Loop ---\n",
        "def train_iteration(model, loader, epochs=5):\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for images, masks, _ in loader:\n",
        "            images, masks = images.to(DEVICE, non_blocking=True), masks.to(DEVICE, non_blocking=True)\n",
        "            optimizer_rand.zero_grad(set_to_none=True)\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=amp_dtype, enabled=use_amp):\n",
        "                preds = model(images)\n",
        "                loss = loss_fn(preds, masks)\n",
        "            loss.backward()\n",
        "            optimizer_rand.step()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, max_batches=10):\n",
        "    model.eval()\n",
        "    total_iou = 0.0\n",
        "    for i, (images, masks, _) in enumerate(loader):\n",
        "        if i >= max_batches: break\n",
        "        images, masks = images.to(DEVICE, non_blocking=True), masks.to(DEVICE, non_blocking=True)\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=amp_dtype, enabled=use_amp):\n",
        "            preds = torch.sigmoid(model(images))\n",
        "        inter = (preds * masks).sum()\n",
        "        union = (preds + masks).sum() - inter\n",
        "        total_iou += (inter / (union + 1e-6)).item()\n",
        "    return total_iou / (i + 1)\n",
        "\n",
        "NUM_ITERATIONS, BATCH_SIZE, EPOCHS_PER_ITER = 15, 20, 5\n",
        "test_loader = make_loader(test_paths, val_transform, batch_size=128)\n",
        "print(f\"Starting baseline experiment from iteration: {start_iter_rand + 1}\")\n",
        "\n",
        "for i in range(start_iter_rand, NUM_ITERATIONS):\n",
        "    print(f\"\\n[Baseline] Iteration {i+1}/{NUM_ITERATIONS} | Labeled: {len(rand_labeled_pool)}\")\n",
        "    train_loader = make_loader(rand_labeled_pool, train_transform, batch_size=64, shuffle=True)\n",
        "    train_iteration(model_rand, train_loader, epochs=EPOCHS_PER_ITER)\n",
        "\n",
        "    test_iou = validate(model_rand, test_loader)\n",
        "    random_history.append({'num_labeled': len(rand_labeled_pool), 'iou': test_iou})\n",
        "    print(f\"  Test IoU: {test_iou:.4f}\")\n",
        "\n",
        "    if not rand_unlabeled_pool:\n",
        "        print(\"Unlabeled pool exhausted.\")\n",
        "        break\n",
        "\n",
        "    new_batch = random.sample(rand_unlabeled_pool, k=min(BATCH_SIZE, len(rand_unlabeled_pool)))\n",
        "    rand_labeled_pool.extend(new_batch)\n",
        "    rand_unlabeled_pool = [p for p in rand_unlabeled_pool if p not in new_batch]\n",
        "\n",
        "    torch.save({\n",
        "        'iter_idx': i, 'model_state': model_rand.state_dict(), 'optimizer_state': optimizer_rand.state_dict(),\n",
        "        'rand_labeled_pool': rand_labeled_pool, 'rand_unlabeled_pool': rand_unlabeled_pool, 'random_history': random_history\n",
        "    }, f\"{CKPT_DIR}/rand_ckpt_{i}.pt\")\n",
        "\n",
        "    del train_loader; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nBaseline experiment complete.\")"
      ],
      "metadata": {
        "id": "Ht8xmMIYBbTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# RESULTS VISUALIZATION\n",
        "# =============================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Generating final comparison plot...\")\n",
        "\n",
        "# --- Load Experiment Histories ---\n",
        "al_ckpt_files = sorted(glob.glob(f\"{CKPT_DIR}/al_ckpt_*.pt\"), key=os.path.getmtime, reverse=True)\n",
        "rand_ckpt_files = sorted(glob.glob(f\"{CKPT_DIR}/rand_ckpt_*.pt\"), key=os.path.getmtime, reverse=True)\n",
        "\n",
        "active_learning_history = torch.load(al_ckpt_files[0])['active_learning_history'] if al_ckpt_files else []\n",
        "random_history = torch.load(rand_ckpt_files[0])['random_history'] if rand_ckpt_files else []\n",
        "\n",
        "# --- Plotting ---\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "if active_learning_history:\n",
        "    al_df = pd.DataFrame(active_learning_history)\n",
        "    ax.plot(al_df['num_labeled'], al_df['iou'], marker='o', linestyle='-', color='crimson', label='Active Learning (HDBSCAN Outlier)')\n",
        "\n",
        "if random_history:\n",
        "    rand_df = pd.DataFrame(random_history)\n",
        "    ax.plot(rand_df['num_labeled'], rand_df['iou'], marker='s', linestyle='--', color='dodgerblue', label='Random Sampling (Baseline)')\n",
        "\n",
        "ax.set_title('Active Learning vs. Random Sampling Performance', fontsize=16, fontweight='bold')\n",
        "ax.set_xlabel('Number of Labeled Images', fontsize=12)\n",
        "ax.set_ylabel('Test Set Intersection over Union (IoU)', fontsize=12)\n",
        "ax.axhline(y=0.6, color='gray', linestyle=':', linewidth=2, label='Target IoU (0.60)')\n",
        "ax.legend(fontsize=12)\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BxChB9pOBczo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
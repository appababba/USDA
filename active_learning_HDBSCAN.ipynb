{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3pd5xeBLRmlzQsy7ajp7j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appababba/USDA/blob/main/active_learning_HDBSCAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo8WmJau_MEU"
      },
      "outputs": [],
      "source": [
        "!pip install -q segmentation-models-pytorch==0.3.3 albumentations==1.4.7 hdbscan --no-deps\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import math\n",
        "import glob\n",
        "import gc\n",
        "import pickle\n",
        "import json\n",
        "import copy\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from contextlib import nullcontext\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "import hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Environment and Reproducibility ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "if hasattr(torch.backends, \"cuda\") and hasattr(torch.backends.cuda, \"matmul\"):\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "if hasattr(torch.backends, \"cudnn\"):\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# --- Automatic Mixed Precision (AMP) Setup ---\n",
        "use_amp = torch.cuda.is_available() and hasattr(torch.cuda, 'amp') and torch.cuda.is_bf16_supported()\n",
        "amp_dtype = torch.bfloat16 if use_amp else torch.float32\n",
        "\n",
        "# --- Google Drive Integration ---\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting drive: {e}\")"
      ],
      "metadata": {
        "id": "1dkE82R7_rtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Path Definitions ---\n",
        "BASE_DRIVE = \"/content/drive/Shared drives/USDA-Summer2025\"\n",
        "DATA_DIR   = os.path.join(BASE_DRIVE, \"data\")\n",
        "IMG_DRIVE  = os.path.join(DATA_DIR, \"Exported_Images\")\n",
        "MSK_DRIVE  = os.path.join(DATA_DIR, \"Exported_Masks\")\n",
        "MODELS_DIR = os.path.join(BASE_DRIVE, \"models\")\n",
        "LOCAL_ROOT = \"/content/local_data\"\n",
        "IMG_LOCAL  = os.path.join(LOCAL_ROOT, \"Exported_Images\")\n",
        "MSK_LOCAL  = os.path.join(LOCAL_ROOT, \"Exported_Masks\")\n",
        "os.makedirs(LOCAL_ROOT, exist_ok=True)\n",
        "\n",
        "# --- Local Data Mirroring ---\n",
        "def ensure_local_data(source_dir, destination_dir):\n",
        "    \"\"\"Mirrors data from a source (e.g., G-Drive) to a local directory for performance.\"\"\"\n",
        "    if os.path.isdir(destination_dir) and any(os.scandir(destination_dir)):\n",
        "        print(f\"Local data found at: {destination_dir}\")\n",
        "        return\n",
        "    print(f\"Copying data from {source_dir} to {destination_dir}...\")\n",
        "    shutil.copytree(source_dir, destination_dir)\n",
        "    print(\"Data copy complete.\")\n",
        "\n",
        "ensure_local_data(IMG_DRIVE, IMG_LOCAL)\n",
        "ensure_local_data(MSK_DRIVE, MSK_LOCAL)"
      ],
      "metadata": {
        "id": "MN56bG9K_tnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PyTorch Dataset Definitions ---\n",
        "class SegDataset(Dataset):\n",
        "    def __init__(self, image_paths, mask_dir, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def _get_mask_path(self, img_path):\n",
        "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        # Check for primary mask naming convention\n",
        "        primary_mask_path = os.path.join(self.mask_dir, f\"{base_name}_mask.png\")\n",
        "        if os.path.exists(primary_mask_path):\n",
        "            return primary_mask_path\n",
        "        # Check for alternative extensions\n",
        "        for ext in ('.png', '.jpg', '.jpeg', '.tif', '.tiff'):\n",
        "            alt_mask_path = os.path.join(self.mask_dir, base_name + ext)\n",
        "            if os.path.exists(alt_mask_path):\n",
        "                return alt_mask_path\n",
        "        raise FileNotFoundError(f\"No corresponding mask found for image: {img_path}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        mask_path = self._get_mask_path(img_path)\n",
        "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "        mask = (cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) > 0).astype(np.uint8)\n",
        "\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, mask=mask)\n",
        "            image, mask = transformed[\"image\"], transformed[\"mask\"]\n",
        "\n",
        "        if isinstance(mask, torch.Tensor) and mask.ndim == 2:\n",
        "            mask = mask.unsqueeze(0)\n",
        "        return image, mask.float(), img_path\n",
        "\n",
        "class InferenceDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        return image, img_path\n",
        "\n",
        "# --- Model Architecture ---\n",
        "class GaborStem(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.k, self.gamma, self.magnitude = cfg[\"kernel_size\"], cfg[\"gamma\"], cfg[\"magnitude\"]\n",
        "        ax = torch.arange(-(self.k // 2), self.k // 2 + 1, dtype=torch.float32)\n",
        "        X, Y = torch.meshgrid(ax, ax, indexing='xy')\n",
        "        self.register_buffer('X', X); self.register_buffer('Y', Y)\n",
        "\n",
        "        params = []\n",
        "        for lam in cfg[\"wavelengths\"]:\n",
        "            for sig in cfg[\"sigmas\"]:\n",
        "                for th in torch.linspace(0, math.pi, steps=cfg[\"orientations\"]):\n",
        "                    for ph in cfg[\"phases\"]:\n",
        "                        params.append((th.item(), sig, lam, ph))\n",
        "\n",
        "        self.register_buffer('params_buf', torch.tensor(params, dtype=torch.float32))\n",
        "        self.register_buffer('phases_buf', torch.tensor(cfg[\"phases\"], dtype=torch.float32))\n",
        "\n",
        "        num_filters = len(cfg[\"wavelengths\"]) * len(cfg[\"sigmas\"]) * cfg[\"orientations\"]\n",
        "        self.out_channels = num_filters if self.magnitude and len(cfg[\"phases\"]) == 2 else len(params)\n",
        "        self.norm = nn.InstanceNorm2d(self.out_channels, affine=False)\n",
        "\n",
        "    def _get_kernels(self, device, dtype):\n",
        "        P, X, Y = self.params_buf.to(device, dtype), self.X.to(device, dtype), self.Y.to(device, dtype)\n",
        "        gamma = torch.as_tensor(self.gamma, dtype=dtype, device=device)\n",
        "        theta, sigma, lambd, phase = [p.view(-1, 1, 1) for p in P.T]\n",
        "\n",
        "        Xp = X * torch.cos(theta) + Y * torch.sin(theta)\n",
        "        Yp = -X * torch.sin(theta) + Y * torch.cos(theta)\n",
        "        gauss = torch.exp(-(Xp**2 + (gamma * Yp)**2) / (2 * sigma**2))\n",
        "        carrier = torch.cos(2 * math.pi * Xp / lambd + phase)\n",
        "        g = (gauss * carrier)\n",
        "        g -= g.mean(dim=[1, 2], keepdim=True)\n",
        "        g /= (g.norm(p=2, dim=[1, 2], keepdim=True) + 1e-8)\n",
        "        return g\n",
        "\n",
        "    def forward(self, x):\n",
        "        device, dtype = x.device, x.dtype\n",
        "        luminance = 0.299 * x[:, 0:1] + 0.587 * x[:, 1:2] + 0.114 * x[:, 2:3]\n",
        "        K = self._get_kernels(device, dtype)\n",
        "\n",
        "        if self.magnitude and self.phases_buf.numel() == 2:\n",
        "            rc = F.conv2d(luminance, K[0::2].unsqueeze(1), padding=self.k // 2)\n",
        "            rs = F.conv2d(luminance, K[1::2].unsqueeze(1), padding=self.k // 2)\n",
        "            feats = torch.sqrt(rc**2 + rs**2 + 1e-8)\n",
        "        else:\n",
        "            feats = F.conv2d(luminance, K.unsqueeze(1), padding=self.k // 2)\n",
        "        return self.norm(feats)\n",
        "\n",
        "class UNetWithGabor_Adapted(nn.Module):\n",
        "    def __init__(self, gcfg, in_img_ch=3):\n",
        "        super().__init__()\n",
        "        self.gabor = GaborStem(gcfg) if gcfg.get(\"enabled\", False) else None\n",
        "        self.mode = gcfg.get(\"mode\", \"concat\")\n",
        "\n",
        "        g_out_ch = self.gabor.out_channels if self.gabor else 0\n",
        "        adapter_in_ch = g_out_ch\n",
        "        if self.gabor and self.mode == \"concat\":\n",
        "            adapter_in_ch += in_img_ch\n",
        "\n",
        "        self.adapter = nn.Identity() if adapter_in_ch <= 3 else nn.Sequential(\n",
        "            nn.Conv2d(adapter_in_ch, max(16, adapter_in_ch // 4), kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(max(16, adapter_in_ch // 4)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(max(16, adapter_in_ch // 4), 3, kernel_size=1, bias=False)\n",
        "        )\n",
        "        self.net = smp.Unet('resnet50', encoder_weights='imagenet', in_channels=3, classes=1)\n",
        "\n",
        "    def _prepare_input(self, x):\n",
        "        if not self.gabor:\n",
        "            return self.adapter(x)\n",
        "        g_feats = self.gabor(x)\n",
        "        if self.mode == \"concat\":\n",
        "            x = torch.cat([x, g_feats], dim=1)\n",
        "        else:\n",
        "            x = g_feats\n",
        "        return self.adapter(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(self._prepare_input(x))\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        x = self._prepare_input(x)\n",
        "        # Return the output of the last encoder stage\n",
        "        return self.net.encoder(x)[-1]"
      ],
      "metadata": {
        "id": "XVZ2pbY-_vmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration and Utilities ---\n",
        "GABOR_CFG_INFERENCE = {\n",
        "    \"enabled\": True, \"mode\": \"concat\", \"kernel_size\": 15, \"orientations\": 8,\n",
        "    \"wavelengths\": [4.0, 8.0, 12.0], \"sigmas\": [1.5, 3.0, 4.5],\n",
        "    \"phases\": [0, np.pi/2], \"gamma\": 0.5, \"magnitude\": True\n",
        "}\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# --- Model Initialization and Weight Loading ---\n",
        "print(\"Initializing model...\")\n",
        "model = UNetWithGabor_Adapted(gcfg=GABOR_CFG_INFERENCE).to(DEVICE, memory_format=torch.channels_last)\n",
        "\n",
        "model_path = os.path.join(MODELS_DIR, \"UNetRes50_GaborAdapter_imagenet.pth\")\n",
        "print(f\"Loading weights from: {model_path}\")\n",
        "try:\n",
        "    ckpt = torch.load(model_path, map_location=DEVICE)\n",
        "    state_dict = ckpt.get(\"state_dict\", ckpt)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "    print(\"Model weights loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading weights: {e}\")\n",
        "\n",
        "# --- Model Compilation (Best-Effort) ---\n",
        "try:\n",
        "    print(\"Compiling model for performance...\")\n",
        "    model = torch.compile(model, mode=\"max-autotune\")\n",
        "    print(\"Model compiled successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"torch.compile failed or is not supported: {e}\")\n",
        "\n",
        "\n",
        "# --- Feature Extraction ---\n",
        "all_image_paths = [os.path.join(IMG_LOCAL, f) for f in os.listdir(IMG_LOCAL) if not f.startswith('.')]\n",
        "print(f\"\\nFound {len(all_image_paths)} total images for feature extraction.\")\n",
        "\n",
        "inference_dataset = InferenceDataset(all_image_paths, transform=val_transform)\n",
        "inference_loader = DataLoader(inference_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=(DEVICE.type == \"cuda\"))\n",
        "\n",
        "all_features, all_paths = [], []\n",
        "with torch.no_grad():\n",
        "    for images, paths in tqdm(inference_loader, desc=\"Extracting Features\"):\n",
        "        images = images.to(DEVICE, non_blocking=True)\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=amp_dtype, enabled=use_amp):\n",
        "            features = model.extract_features(images)\n",
        "            pooled_features = F.adaptive_avg_pool2d(features, (1, 1)).flatten(1)\n",
        "            normalized_features = F.normalize(pooled_features, p=2, dim=1)\n",
        "        all_features.append(normalized_features.to(torch.float32).cpu().numpy())\n",
        "        all_paths.extend(paths)\n",
        "\n",
        "feature_vectors = np.vstack(all_features)\n",
        "print(\"Feature extraction complete.\")\n",
        "print(f\"Feature vector shape: {feature_vectors.shape}, Total paths: {len(all_paths)}\")"
      ],
      "metadata": {
        "id": "HxEXIVdx_wqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clustering with HDBSCAN ---\n",
        "print(\"\\nPerforming HDBSCAN clustering on feature vectors...\")\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, prediction_data=True, core_dist_n_jobs=-1)\n",
        "labels = clusterer.fit_predict(feature_vectors)\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise = np.sum(labels == -1)\n",
        "print(f\"Clustering complete. Found {n_clusters} clusters and {n_noise} noise points (outliers).\")\n",
        "\n",
        "# --- Identify and Rank Informative Samples ---\n",
        "outlier_scores = clusterer.outlier_scores_\n",
        "scored_samples = sorted(zip(outlier_scores, all_paths), key=lambda x: x[0], reverse=True)\n",
        "\n",
        "def select_next_batch(scored_samples, already_labeled_paths, batch_size=20):\n",
        "    next_batch, labeled_set = [], set(already_labeled_paths)\n",
        "    for _, path in scored_samples:\n",
        "        if path not in labeled_set:\n",
        "            next_batch.append(path)\n",
        "            if len(next_batch) >= batch_size:\n",
        "                break\n",
        "    return next_batch\n",
        "\n",
        "# --- Select Initial Batch for Labeling ---\n",
        "labeled_image_paths = []\n",
        "BATCH_SIZE = 20\n",
        "first_batch_to_label = select_next_batch(scored_samples, labeled_image_paths, batch_size=BATCH_SIZE)\n",
        "print(f\"\\nSelected the top {len(first_batch_to_label)} most informative samples to label next:\")\n",
        "for i, p in enumerate(first_batch_to_label[:5], 1):\n",
        "    print(f\"  {i}. {os.path.basename(p)}\")"
      ],
      "metadata": {
        "id": "b4FBMisq_y0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Initializing Active Learning Loop ---\")\n",
        "CKPT_DIR = '/content/drive/MyDrive/active_learning_ckpts'\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "NUM_WORKERS = 0\n",
        "PIN_MEMORY = (DEVICE.type == \"cuda\")\n",
        "\n",
        "# --- Resume-Safe Checkpoint Loading ---\n",
        "ckpt_files = sorted(glob.glob(f\"{CKPT_DIR}/al_ckpt_*.pt\"), key=os.path.getmtime, reverse=True)\n",
        "latest_ckpt_path = ckpt_files[0] if ckpt_files else None\n",
        "\n",
        "current_model = UNetWithGabor_Adapted(gcfg=GABOR_CFG_INFERENCE).to(DEVICE, memory_format=torch.channels_last)\n",
        "optimizer = optim.Adam(current_model.parameters(), lr=1e-4)\n",
        "\n",
        "if latest_ckpt_path:\n",
        "    print(f\"Resuming from checkpoint: {os.path.basename(latest_ckpt_path)}\")\n",
        "    ckpt = torch.load(latest_ckpt_path, map_location=DEVICE)\n",
        "    current_model.load_state_dict(ckpt['model_state'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer_state'])\n",
        "    al_labeled_pool = ckpt['al_labeled_pool']\n",
        "    al_unlabeled_pool = ckpt['al_unlabeled_pool']\n",
        "    active_learning_history = ckpt['active_learning_history']\n",
        "    start_iter = ckpt['iter_idx'] + 1\n",
        "else:\n",
        "    print(\"No existing checkpoint found. Creating initial stratified data pools...\")\n",
        "    start_iter = 0\n",
        "    FULL_DATASET_PATHS = [os.path.join(IMG_LOCAL, f) for f in os.listdir(IMG_LOCAL) if not f.startswith('.')]\n",
        "    all_ratios = []\n",
        "    helper_dataset = SegDataset([], MSK_LOCAL)\n",
        "    for path in tqdm(FULL_DATASET_PATHS, desc=\"Stratifying Data by Mask Ratio\"):\n",
        "        try:\n",
        "            mask_path = helper_dataset._get_mask_path(path)\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "            ratio = np.mean(mask > 0) if mask is not None else 0.0\n",
        "            all_ratios.append(ratio)\n",
        "        except FileNotFoundError:\n",
        "            all_ratios.append(0.0)\n",
        "\n",
        "    df = pd.DataFrame({'path': FULL_DATASET_PATHS, 'ratio': all_ratios})\n",
        "    df['ratio_bin'] = pd.qcut(df['ratio'], q=5, labels=False, duplicates='drop')\n",
        "\n",
        "    train_val_paths, test_paths, _, _ = train_test_split(\n",
        "        df['path'].tolist(), df['ratio_bin'].tolist(),\n",
        "        test_size=0.2, random_state=SEED, stratify=df['ratio_bin'].tolist()\n",
        "    )\n",
        "    random.shuffle(train_val_paths)\n",
        "    al_labeled_pool = train_val_paths[:20]\n",
        "    al_unlabeled_pool = train_val_paths[20:]\n",
        "    active_learning_history = []\n",
        "\n",
        "if 'test_paths' not in locals():\n",
        "     _, test_paths = train_test_split([os.path.join(IMG_LOCAL, f) for f in os.listdir(IMG_LOCAL)], test_size=0.2, random_state=SEED)\n",
        "\n",
        "# --- DataLoaders and Training/Validation Functions ---\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(256, 256), A.HorizontalFlip(p=0.5), A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2(),\n",
        "])\n",
        "def make_loader(paths, transform, batch_size, shuffle=False):\n",
        "    ds = SegDataset(paths, MSK_LOCAL, transform=transform)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "def train_one_iteration(model, loader, epochs=5):\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for images, masks, _ in loader:\n",
        "            images, masks = images.to(DEVICE, non_blocking=True), masks.to(DEVICE, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=amp_dtype, enabled=use_amp):\n",
        "                preds = model(images)\n",
        "                loss = nn.BCEWithLogitsLoss()(preds, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_performance(model, loader, max_batches=10):\n",
        "    if not loader.dataset: return 0.0\n",
        "    model.eval()\n",
        "    total_iou, n_batches = 0.0, 0\n",
        "    for i, (images, masks, _) in enumerate(loader):\n",
        "        if i >= max_batches: break\n",
        "        images, masks = images.to(DEVICE, non_blocking=True), masks.to(DEVICE, non_blocking=True)\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=amp_dtype, enabled=use_amp):\n",
        "            preds = torch.sigmoid(model(images))\n",
        "        inter = (preds * masks).sum()\n",
        "        union = (preds + masks).sum() - inter\n",
        "        total_iou += (inter / (union + 1e-6)).item()\n",
        "        n_batches += 1\n",
        "    return total_iou / max(1, n_batches)\n",
        "\n",
        "# --- Fast Batch Selection with Caching ---\n",
        "EMB_CACHE = {}\n",
        "PCA_MODEL = None\n",
        "@torch.no_grad()\n",
        "def get_embeddings_for_paths(paths, batch_size=64):\n",
        "    to_compute = [p for p in paths if p not in EMB_CACHE]\n",
        "    if to_compute:\n",
        "        ds = InferenceDataset(to_compute, transform=val_transform)\n",
        "        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "        current_model.eval()\n",
        "        for images, computed_paths in loader:\n",
        "            images = images.to(DEVICE)\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=amp_dtype, enabled=use_amp):\n",
        "                feats = F.normalize(F.adaptive_avg_pool2d(current_model.extract_features(images), (1, 1)).flatten(1))\n",
        "            for p, f in zip(computed_paths, feats.cpu().numpy()):\n",
        "                EMB_CACHE[p] = f\n",
        "    return np.stack([EMB_CACHE[p] for p in paths])\n",
        "\n",
        "def select_next_batch_fast(unlabeled_paths, batch_size=20, candidate_k=500):\n",
        "    global PCA_MODEL\n",
        "    if not unlabeled_paths: return []\n",
        "    candidates = random.sample(unlabeled_paths, k=min(candidate_k, len(unlabeled_paths)))\n",
        "    feats = get_embeddings_for_paths(candidates)\n",
        "\n",
        "    if PCA_MODEL is None:\n",
        "        PCA_MODEL = PCA(n_components=64, random_state=SEED).fit(feats)\n",
        "    reduced_feats = PCA_MODEL.transform(feats)\n",
        "\n",
        "    scores = hdbscan.HDBSCAN(min_cluster_size=15).fit(reduced_feats).outlier_scores_\n",
        "    ranked = [c for _, c in sorted(zip(scores, candidates), key=lambda x: x[0], reverse=True)]\n",
        "    return ranked[:batch_size]\n",
        "\n",
        "# --- Main Loop Execution ---\n",
        "NUM_ITERATIONS = 15\n",
        "EPOCHS_PER_ITER = 5\n",
        "print(f\"Starting active learning from iteration: {start_iter + 1}\")\n",
        "\n",
        "for i in range(start_iter, NUM_ITERATIONS):\n",
        "    num_labeled = len(al_labeled_pool)\n",
        "    print(f\"\\n--- Iteration {i+1}/{NUM_ITERATIONS} | Labeled Samples: {num_labeled} ---\")\n",
        "\n",
        "    train_loader = make_loader(al_labeled_pool, train_transform, batch_size=32, shuffle=True)\n",
        "    train_one_iteration(current_model, train_loader, epochs=EPOCHS_PER_ITER)\n",
        "\n",
        "    test_loader = make_loader(test_paths, val_transform, batch_size=64)\n",
        "    test_iou = validate_performance(current_model, test_loader)\n",
        "    active_learning_history.append({'num_labeled': num_labeled, 'iou': test_iou})\n",
        "    print(f\"Validation IoU: {test_iou:.4f}\")\n",
        "\n",
        "    if not al_unlabeled_pool:\n",
        "        print(\"Unlabeled pool is empty. Stopping.\")\n",
        "        break\n",
        "\n",
        "    print(\"Selecting next batch of samples...\")\n",
        "    new_batch = select_next_batch_fast(al_unlabeled_pool, batch_size=BATCH_SIZE)\n",
        "\n",
        "    al_labeled_pool.extend(new_batch)\n",
        "    al_unlabeled_pool = [p for p in al_unlabeled_pool if p not in new_batch]\n",
        "\n",
        "    torch.save({\n",
        "        'iter_idx': i, 'model_state': current_model.state_dict(),\n",
        "        'optimizer_state': optimizer.state_dict(), 'al_labeled_pool': al_labeled_pool,\n",
        "        'al_unlabeled_pool': al_unlabeled_pool, 'active_learning_history': active_learning_history\n",
        "    }, f\"{CKPT_DIR}/al_ckpt_{i}.pt\")\n",
        "\n",
        "    del train_loader, test_loader; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\nActive learning complete. Checkpoints saved in: {CKPT_DIR}\")"
      ],
      "metadata": {
        "id": "zzEjCuqm_3n2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}